{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifier Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.tree import DecisionTreeClassifier\n",
    "from src.ensemble import BaggingClassifier\n",
    "from src.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    return pd.read_csv(filename, sep = \";\")\n",
    "\n",
    "def split_input_output(data, target_column):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_train_test(X, y, test_size, random_state=42):\n",
    "    return train_test_split(X, y,\n",
    "                            test_size = test_size,\n",
    "                            stratify = y,\n",
    "                            random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE THE DATA\n",
    "# -----------------\n",
    "# Load data\n",
    "filename = \"../../data/raw/bank.csv\"\n",
    "data = load_data(filename)\n",
    "\n",
    "# Filter data\n",
    "data = data[[\"age\", \"balance\", \"duration\", \"day\", \"y\"]]\n",
    "data[\"y\"] = np.where(data[\"y\"] == \"no\", -1, 1)\n",
    "\n",
    "# Split data\n",
    "X, y = split_input_output(data, \"y\")\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = split_train_test(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "-------------\n",
      "acc. train  : 100.00%\n",
      "acc. valid  : 82.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TREE MODELING\n",
    "# -------------\n",
    "# Create Decision Tree Classifier\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_tree = clf_tree.predict(X_train)\n",
    "y_pred_valid_tree = clf_tree.predict(X_valid)\n",
    "\n",
    "acc_train_tree = accuracy_score(y_train, y_pred_train_tree)\n",
    "acc_valid_tree = accuracy_score(y_valid, y_pred_valid_tree)\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(\"-------------\")\n",
    "print(f\"acc. train  : {acc_train_tree*100:.2f}%\")\n",
    "print(f\"acc. valid  : {acc_valid_tree*100:.2f}%\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Tree\n",
      "-------------\n",
      "acc. train  : 98.55%\n",
      "acc. valid  : 87.29%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BAGGING MODELING\n",
    "# -------------\n",
    "# Create Bagging Classifier\n",
    "clf_bagging = BaggingClassifier(estimator = DecisionTreeClassifier(),\n",
    "                                n_estimators = 10,\n",
    "                                random_state = 42)\n",
    "clf_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_bagging = clf_bagging.predict(X_train)\n",
    "y_pred_valid_bagging = clf_bagging.predict(X_valid)\n",
    "\n",
    "acc_train_bagging = accuracy_score(y_train, y_pred_train_bagging)\n",
    "acc_valid_bagging = accuracy_score(y_valid, y_pred_valid_bagging)\n",
    "\n",
    "print(\"Bagging Tree\")\n",
    "print(\"-------------\")\n",
    "print(f\"acc. train  : {acc_train_bagging*100:.2f}%\")\n",
    "print(f\"acc. valid  : {acc_valid_bagging*100:.2f}%\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Tree\n",
      "-------------\n",
      "acc. train  : 97.06%\n",
      "acc. valid  : 88.40%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST MODELING\n",
    "# -------------\n",
    "# Create Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(n_estimators = 10,\n",
    "                                random_state = 42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_rf = clf_rf.predict(X_train)\n",
    "y_pred_valid_rf = clf_rf.predict(X_valid)\n",
    "\n",
    "acc_train_rf = accuracy_score(y_train, y_pred_train_rf)\n",
    "acc_valid_rf = accuracy_score(y_valid, y_pred_valid_rf)\n",
    "\n",
    "print(\"Random Forest Tree\")\n",
    "print(\"-------------\")\n",
    "print(f\"acc. train  : {acc_train_rf*100:.2f}%\")\n",
    "print(f\"acc. valid  : {acc_valid_rf*100:.2f}%\")\n",
    "print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.tree import DecisionTreeRegressor\n",
    "from src.ensemble import BaggingRegressor\n",
    "from src.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    return pd.read_csv(filename, sep = \",\")\n",
    "\n",
    "def split_input_output(data, target_column):\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def split_train_test(X, y, test_size, random_state=42):\n",
    "    return train_test_split(X, y,\n",
    "                            test_size = test_size,\n",
    "                            random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE THE DATA\n",
    "# -----------------\n",
    "# Load data\n",
    "filename = \"../../data/raw/auto.csv\"\n",
    "data = load_data(filename)\n",
    "\n",
    "# Filter data\n",
    "data = data[[\"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"mpg\"]]\n",
    "\n",
    "# Split data\n",
    "X, y = split_input_output(data, \"mpg\")\n",
    "X_train, X_test, y_train, y_test = split_train_test(X, y, test_size=0.2)\n",
    "X_train, X_valid, y_train, y_valid = split_train_test(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "-------------\n",
      "MSE train  : 0.0180\n",
      "MSE valid  : 21.3168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TREE MODELING\n",
    "# -------------\n",
    "# Create Decision Tree Regressor\n",
    "clf_tree = DecisionTreeRegressor()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_tree = clf_tree.predict(X_train)\n",
    "y_pred_valid_tree = clf_tree.predict(X_valid)\n",
    "\n",
    "mse_train_tree = mean_squared_error(y_train, y_pred_train_tree)\n",
    "mse_valid_tree = mean_squared_error(y_valid, y_pred_valid_tree)\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print(\"-------------\")\n",
    "print(f\"MSE train  : {mse_train_tree:.4f}\")\n",
    "print(f\"MSE valid  : {mse_valid_tree:.4f}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Tree\n",
      "-------------\n",
      "MSE train  : 2.3199\n",
      "MSE valid  : 15.1881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BAGGING MODELING\n",
    "# -------------\n",
    "# Create Bagging Regressor\n",
    "clf_bagging = BaggingRegressor(estimator = DecisionTreeRegressor(),\n",
    "                                n_estimators = 30,\n",
    "                                random_state = 42)\n",
    "clf_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_bagging = clf_bagging.predict(X_train)\n",
    "y_pred_valid_bagging = clf_bagging.predict(X_valid)\n",
    "\n",
    "mse_train_bagging = mean_squared_error(y_train, y_pred_train_bagging)\n",
    "mse_valid_bagging = mean_squared_error(y_valid, y_pred_valid_bagging)\n",
    "\n",
    "print(\"Bagging Tree\")\n",
    "print(\"-------------\")\n",
    "print(f\"MSE train  : {mse_train_bagging:.4f}\")\n",
    "print(f\"MSE valid  : {mse_valid_bagging:.4f}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# RANDOM FOREST MODELING\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# -------------\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Create Random Forest Regressor\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m clf_rf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                                 random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m clf_rf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Predict the tree\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/st_yudi/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/11-Ensemble-Model.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m y_pred_train_rf \u001b[39m=\u001b[39m clf_rf\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/ensemble/base.py:300\u001b[0m, in \u001b[0;36mBaseEnsemble.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39m# Fit the model from the bootstrapped sample\u001b[39;00m\n\u001b[1;32m    299\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[b]\n\u001b[0;32m--> 300\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X_bootstrap, y_bootstrap)\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:721\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impurity_evaluation \u001b[39m=\u001b[39m CRITERIA_REG[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion]\n\u001b[1;32m    719\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_leaf_value_calculation \u001b[39m=\u001b[39m _calculate_average_vote\n\u001b[0;32m--> 721\u001b[0m \u001b[39msuper\u001b[39;49m(DecisionTreeRegressor, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:261\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    260\u001b[0m \u001b[39m# Grow tree\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grow_tree(X, y)\n\u001b[1;32m    263\u001b[0m \u001b[39m# Prune tree\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prune_tree()\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:339\u001b[0m, in \u001b[0;36mBaseDecisionTree._grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m    336\u001b[0m         node\u001b[39m.\u001b[39mis_leaf \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    338\u001b[0m         \u001b[39m# Grow the tree\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m         node\u001b[39m.\u001b[39mchildren_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grow_tree(X_left, y_left, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    340\u001b[0m         node\u001b[39m.\u001b[39mchildren_right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grow_tree(X_right, y_right, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:318\u001b[0m, in \u001b[0;36mBaseDecisionTree._grow_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# if the max depth not reached\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m cond:\n\u001b[1;32m    316\u001b[0m     \u001b[39m# Find the best feature and feature threshold\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# for splitting the tree\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     feature_i, threshold_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_best_split(X, y)\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m feature_i \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         \u001b[39m# Split the data\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcolumn_stack((X, y))\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:401\u001b[0m, in \u001b[0;36mBaseDecisionTree._best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    399\u001b[0m cond_2 \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(right_y) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_samples_leaf\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m cond_1 \u001b[39mand\u001b[39;00m cond_2:\n\u001b[0;32m--> 401\u001b[0m     current_gain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calculate_impurity_decrease(y,\n\u001b[1;32m    402\u001b[0m                                                      left_y,\n\u001b[1;32m    403\u001b[0m                                                      right_y)\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m current_gain \u001b[39m>\u001b[39m best_gain:\n\u001b[1;32m    406\u001b[0m         best_gain \u001b[39m=\u001b[39m current_gain\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree._calculate_impurity_decrease\u001b[0;34m(self, parent, left_children, right_children)\u001b[0m\n\u001b[1;32m    455\u001b[0m N_t_R \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(right_children)\n\u001b[1;32m    457\u001b[0m \u001b[39m# Calculate the impurity\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m I_parent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_impurity_evaluation(parent)\n\u001b[1;32m    459\u001b[0m I_child_left \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impurity_evaluation(left_children)\n\u001b[1;32m    460\u001b[0m I_child_right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impurity_evaluation(right_children)\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/criterion.py:94\u001b[0m, in \u001b[0;36mMSE\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     91\u001b[0m node_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Calculate the node-impurity (variance)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m node_impurity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([(y_i \u001b[39m-\u001b[39m node_mean)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m y_i \u001b[39min\u001b[39;00m y])\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m node_impurity\n",
      "File \u001b[0;32m~/portfolio/11_Machine_Learning_From_Scratch/source_code/notebook/../src/tree/criterion.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m node_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y)\n\u001b[1;32m     93\u001b[0m \u001b[39m# Calculate the node-impurity (variance)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m node_impurity \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([(y_i \u001b[39m-\u001b[39;49m node_mean)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39mfor\u001b[39;00m y_i \u001b[39min\u001b[39;00m y])\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m node_impurity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST MODELING\n",
    "# -------------\n",
    "# Create Random Forest Regressor\n",
    "clf_rf = RandomForestRegressor(n_estimators = 10,\n",
    "                                random_state = 42)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the tree\n",
    "y_pred_train_rf = clf_rf.predict(X_train)\n",
    "y_pred_valid_rf = clf_rf.predict(X_valid)\n",
    "\n",
    "mse_train_rf = mean_squared_error(y_train, y_pred_train_rf)\n",
    "mse_valid_rf = mean_squared_error(y_valid, y_pred_valid_rf)\n",
    "\n",
    "print(\"Random Forest Tree\")\n",
    "print(\"------------------\")\n",
    "print(f\"MSE train  : {mse_train_rf:.4f}\")\n",
    "print(f\"MSE valid  : {mse_valid_rf:.4f}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
