{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 10:19:18.031683: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-25 10:19:18.552558: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-25 10:19:18.602119: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-25 10:19:18.602188: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-25 10:19:21.167427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-25 10:19:21.167597: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-25 10:19:21.167605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 10:19:32.861656: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-25 10:19:32.861755: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-25 10:19:32.861786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SAT-PC): /proc/driver/nvidia/version does not exist\n",
      "2023-06-25 10:19:32.862307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "No sentence-transformers model found with name ../../model/indobert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "embedding_nnlm = tf.keras.models.load_model(\"../../model/nnlm-id-dim50/\")\n",
    "embedding_indobert = SentenceTransformer(\"../../model/indobert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_nnlm(model : object, \n",
    "               text : str) -> list :\n",
    "    \"\"\"Function to create embedding \n",
    "        using model object\n",
    "\n",
    "    Args:\n",
    "        model (object): Embedding model\n",
    "        text (str): text input\n",
    "\n",
    "    Returns:\n",
    "        list: embedding of the text input\n",
    "    \"\"\"\n",
    "    try :\n",
    "        embedding_list = model([text]).numpy().squeeze().tolist()\n",
    "        return embedding_list\n",
    "    except : \n",
    "        print(f\"The error text is : {text}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetorize Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source : \n",
    "- https://github.com/kirralabs/indonesian-NLP-resources\n",
    "- https://github.com/sastrawi/sastrawi/blob/master/data/kata-dasar.original.txt\n",
    "- https://github.com/edwardsamuel/Wilayah-Administratif-Indonesia/tree/master/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93566 entries, 0 to 93565\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   words   93566 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 731.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "district_data = pd.read_csv(\"../../data/raw/districts.csv\", header=None)\n",
    "province_data = pd.read_csv(\"../../data/raw/provinces.csv\", header=None)\n",
    "regencies_data = pd.read_csv(\"../../data/raw/regencies.csv\", header=None)\n",
    "villages_data = pd.read_csv(\"../../data/raw/villages.csv\", header=None)\n",
    "kata_data = pd.read_table(\"../../data/raw/kata-dasar.original.txt\", header=None)\n",
    "\n",
    "# format data\n",
    "district_data = district_data[2].apply(lambda x: x.lower())\n",
    "province_data = province_data[1].apply(lambda x: x.lower())\n",
    "regencies_data = regencies_data[2].apply(lambda x: x.lower())\n",
    "villages_data = villages_data[2].apply(lambda x: x.lower())\n",
    "\n",
    "# concat data\n",
    "dataset = pd.concat([district_data, province_data, regencies_data, villages_data, kata_data], axis=0)\n",
    "dataset.columns = [\"words\"]\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error text is : kupan jaya\n"
     ]
    }
   ],
   "source": [
    "# create embedding nnlm\n",
    "dataset_nnlm = dataset.copy()\n",
    "dataset_nnlm['embedding'] = dataset_nnlm['words'].apply(lambda x : embed_text_nnlm(model=embedding_nnlm, text=x))\n",
    "joblib.dump(dataset_nnlm,\"../../data/processed/word_embedding_nnlm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/processed/y_nnlm.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nnlm = joblib.load(\"../../data/processed/word_embedding_nnlm.pkl\")\n",
    "\n",
    "## create train and test data\n",
    "X_nnlm = dataset_nnlm['embedding']\n",
    "y_nnlm = dataset_nnlm['words']\n",
    "\n",
    "## format data train and numpy array\n",
    "X_nnlm =  np.array(X_nnlm.to_numpy().tolist())\n",
    "y_nnlm = y_nnlm.values.astype('str')\n",
    "\n",
    "# dumpy daata into X_nnlm.pkl and y_nnlm.pkl\n",
    "joblib.dump(X_nnlm, \"../../data/processed/X_nnlm.pkl\")\n",
    "joblib.dump(y_nnlm, \"../../data/processed/y_nnlm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30342 entries, 0 to 30341\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   words   30341 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 237.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# create embedding indobert\n",
    "dataset_indobert = kata_data.copy()\n",
    "dataset_indobert.columns = ['words']\n",
    "dataset_indobert.drop_duplicates(inplace=True)\n",
    "dataset_indobert.dropna(inplace=True)\n",
    "dataset_indobert.reset_index(drop=True, inplace=True)\n",
    "dataset_indobert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_indobert(model, text) :\n",
    "    try :\n",
    "        return model.encode(text).tolist()\n",
    "    except :\n",
    "        print(f\"Text causing error : {text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text causing error : nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../data/processed/word_embedding_indobert.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_indobert['embedding'] = dataset_indobert['words'].apply(lambda x : embed_indobert(model=embedding_indobert, text=x))\n",
    "joblib.dump(dataset_indobert, \"../../data/processed/word_embedding_indobert.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
